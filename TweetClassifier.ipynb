{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data IIb: Building a tweet classifier\n",
    "\n",
    "Name: Jocelyn Shen\n",
    "\n",
    "In this project, you'll finish off all the work that we've done so far, and build a collection of classifiers that \n",
    "\n",
    "## 1: Reshaping your code into classes\n",
    "\n",
    "At this point, you should have a solid understanding of text preprocessing, and the Naive Bayes algorithm.  Take the code from your previous projects, and consolidate it into two Python classes: \n",
    "\n",
    "* `TextPreprocessor`: Given a Pandas series of tweets (with punctuation, links, and other garbage), perform all the preprocessing necessary to construct a Pandas dataframe of vectorized tweets.  It should have some keyword arguments (set to reasonable defaults) that include all of the choices that you might make regarding text processing.  Here's an example, taken from our class worksheet:\n",
    "    \n",
    "    ```\n",
    "    >>> df = pd.DataFrame({\"Tweets\":[\"Trick or Treat!\",\n",
    "                                     \"One to Two Guesses\",\n",
    "                                     \"Try this one weird trick!\",\n",
    "                                     \"That's weird, you might guess\",\n",
    "                                     \"Can you guess these 10 health tricks?\"]})\n",
    "    >>> vectorized_tweets = TextProcessor(df, N=1000)\n",
    "    >>> vectorized_tweets\n",
    "    ```\n",
    "    \n",
    "|      | \"one\" | \"weird\" | \"trick\" | \"guess\" |\n",
    "|------|------|------|------|------|\n",
    "|   \"Trick or Treat!\"  |  0   |  0   |  1   |  0    |\n",
    "| \"One to Two Guesses\"  |   1  |   0  |  0   |  1    |\n",
    "| \"Try this one weird trick!\"   |   1  |  1   |  1   |   0   |\n",
    "| \"That's weird, you might guess\"   |  0   |  1   |  0   |  1    |\n",
    "| \"Can you guess these 10 health tricks?\"   |   0  |  0   |  1   |  1 |\n",
    "    \n",
    "* `NaiveBayes`: Given a Pandas Dataframe of vectorized tweets (the output of your `TextPreprocessor` class), train a Naive Bayes classifier, which can then be used to classify tweets it hasn't seen before.  \n",
    "\n",
    "    ```\n",
    "    >>> model = NaiveBayes()\n",
    "    >>> model.fit(vectorized_tweets, y)      // y is the column of 1's and 0's, as usual\n",
    "    >>> model.predict(\"Man, @nicholaszufelt is such a great teacher! #brownnoseforlife\")\n",
    "    1\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_circles, make_moons, make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from math import *\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fivethiryeight_colors = {\"blue\": \"#30a2da\", \n",
    "                         \"red\": \"#fc4f30\", \n",
    "                         \"green\": \"#e5ae38\", \n",
    "                         \"yellow\": \"#6d904f\", \n",
    "                         \"gray\": \"#8b8b8b\"}\n",
    "fivethirtyeight_rb = [\"#30a2da\", \"#fc4f30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Moving the Tesla announcement to Wednesday. Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@markpinc @TeslaMotors thanks!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@Reuters Umm...Autobahn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@vicentes obviously wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@Cocoanetics @heiseonline Not actually based o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0        0.0  Moving the Tesla announcement to Wednesday. Ne...\n",
       "1        1.0                     @markpinc @TeslaMotors thanks!\n",
       "2        0.0                           @Reuters Umm...Autobahn?\n",
       "3        0.0                          @vicentes obviously wrong\n",
       "4        0.0  @Cocoanetics @heiseonline Not actually based o..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_class = pd.read_csv(\"tweets_class.txt\", encoding='ISO-8859-1', sep = \"\\t\")\n",
    "tweets_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextProcessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"TextProcessor class takes a dF and the corpus size as parameters\n",
    "    Processeses text\"\"\"\n",
    "    def __init__(self, df, N):\n",
    "        self.all_tweets = df['text'].values\n",
    "        self.df = df\n",
    "        self.CORPUS_SIZE = N\n",
    "        self.common_words = []\n",
    "    \n",
    "    \"\"\"Removes all punctuation from a string\n",
    "    parameters: s, string to remove punctuation from\n",
    "    return: w, word without punctuation\"\"\"\n",
    "    def remove_punctuation(self, s):\n",
    "        w = ''.join(c for c in s if c not in punctuation)\n",
    "        return w\n",
    "    \n",
    "    \"\"\"Makes sure the word contains only characters\n",
    "    parameters: word, word checking for any non-characters\n",
    "    return: True or False, depending on if the word has all characters or not\"\"\"\n",
    "    def all_char(self, word):\n",
    "        for letter in word:\n",
    "            if not letter.isalnum():\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    \"\"\"Stems a given line\n",
    "    parameters: line, line to stem\n",
    "    return: stemmed_line, the stemmed line\"\"\"\n",
    "    def stem_a_line(self, line,stemmed_words=None, stop_words=None):  \n",
    "        stemmer = PorterStemmer()\n",
    "        words = re.split(\"\\s\", line)\n",
    "        words = [word for word in words if (len(word) > 0 and self.all_char(word))]\n",
    "        stemmed_line = []\n",
    "        for word in words:\n",
    "            temp = word.replace(\"\\n\", \"\").lower()\n",
    "            if \"https\" not in temp and temp[0] != \"@\":\n",
    "                w = stemmer.stem(temp)\n",
    "                if w not in stopwords.words('english'):\n",
    "                    stemmed_line.append(w)\n",
    "                    if stemmed_words != None:                                    \n",
    "                        if w not in stemmed_words: stemmed_words.append(w)        \n",
    "                else:\n",
    "                     if stemmed_words != None:                                     \n",
    "                            if w not in stop_words: stop_words.append(w) \n",
    "        if stemmed_words != None:  \n",
    "            return(stemmed_words, stop_words, stemmed_line)\n",
    "        else:\n",
    "            return stemmed_line\n",
    "        \n",
    "    \"\"\"Vectorize tweets\n",
    "    return: DataFrame of all vectorized, N mos common words\"\"\"\n",
    "    def process(self):\n",
    "        textdata = pd.DataFrame({'all_tweets': self.all_tweets})\n",
    "        for i in range(len(self.all_tweets)):\n",
    "            self.all_tweets[i] = str(self.all_tweets[i])\n",
    "            self.all_tweets[i] = ''.join([c for c in self.all_tweets[i] if c not in punctuation])\n",
    "        stemmed_words = []     \n",
    "        stop_words = []       \n",
    "        stemmed_tweets =[]\n",
    "        s = []\n",
    "        for line in self.all_tweets:\n",
    "            stemmed_words, stop_words, stemmed_line = self.stem_a_line(line,stemmed_words, stop_words)\n",
    "            stemmed_tweets = stemmed_tweets + stemmed_line\n",
    "            s.append(stemmed_line)\n",
    "        common_words = [w[0] for w in Counter(stemmed_words).most_common(self.CORPUS_SIZE)]\n",
    "        v = []\n",
    "        for tweet in self.all_tweets:\n",
    "            a = []\n",
    "            for i in range(self.CORPUS_SIZE):\n",
    "                if common_words[i] in tweet or tweet in common_words[i]:\n",
    "                    a.append(1)\n",
    "                else:\n",
    "                    a.append(0)\n",
    "            v.append(a)\n",
    "        for i in range(len(common_words)):\n",
    "            textdata[common_words[i]] = [n[i] for n in v] \n",
    "        self.common_words = common_words\n",
    "        return textdata\n",
    "    \n",
    "    \"\"\"Adds bigrams to the N common words list\"\"\"\n",
    "    def bigram(self, multiplier):\n",
    "        common_words = self.common_words\n",
    "        BIGRAM_MULTIPLIER = 2\n",
    "        bigram_lyrics = [] \n",
    "        for ilyric in range(len(self.df.text)):\n",
    "            bigram = u''  \n",
    "            i_bigram = 0\n",
    "            stemmed_line = self.stem_a_line(self.df.text[ilyric])\n",
    "            for iwd in range(self.CORPUS_SIZE):\n",
    "                if common_words[iwd] in stemmed_line:\n",
    "                    bigram += \"_\"+common_words[iwd]\n",
    "                    i_bigram += 1\n",
    "                    if i_bigram>BIGRAM_MULTIPLIER: \n",
    "                        break\n",
    "            if i_bigram > 1: \n",
    "                stemmed_line.append(bigram)\n",
    "                if bigram not in common_words: \n",
    "                    common_words.append(bigram)   \n",
    "            bigram_lyrics.append(stemmed_line)  \n",
    "        textdata = pd.DataFrame({'all_tweets': self.all_tweets})\n",
    "        stemmed_words = []     \n",
    "        stop_words = []       \n",
    "        stemmed_tweets =[]\n",
    "        s = []\n",
    "        for line in self.all_tweets:\n",
    "            stemmed_words, stop_words, sL = self.stem_a_line(line,stemmed_words, stop_words)\n",
    "            stemmed_tweets = stemmed_tweets + sL\n",
    "            s.append(sL)\n",
    "        v = []\n",
    "        for tweet in self.all_tweets:\n",
    "            a = []\n",
    "            for i in range(len(common_words)):\n",
    "                if common_words[i] in tweet or tweet in common_words[i]:\n",
    "                    a.append(1)\n",
    "                else:\n",
    "                    a.append(0)\n",
    "            v.append(a)\n",
    "        for i in range(len(common_words)):\n",
    "            textdata[common_words[i]] = [n[i] for n in v] \n",
    "        return textdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_tweets</th>\n",
       "      <th>extend</th>\n",
       "      <th>unkar</th>\n",
       "      <th>1949</th>\n",
       "      <th>demonstr</th>\n",
       "      <th>lit</th>\n",
       "      <th>risk</th>\n",
       "      <th>vc</th>\n",
       "      <th>asshol</th>\n",
       "      <th>goal</th>\n",
       "      <th>...</th>\n",
       "      <th>nerdsunit</th>\n",
       "      <th>signup</th>\n",
       "      <th>mute</th>\n",
       "      <th>raven</th>\n",
       "      <th>1julieanderson</th>\n",
       "      <th>procur</th>\n",
       "      <th>http</th>\n",
       "      <th>hottest</th>\n",
       "      <th>lschibi</th>\n",
       "      <th>bringi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Moving the Tesla announcement to Wednesday. Ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@markpinc @TeslaMotors thanks!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Reuters Umm...Autobahn?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@vicentes obviously wrong</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Cocoanetics @heiseonline Not actually based o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          all_tweets  extend  unkar  1949  \\\n",
       "0  Moving the Tesla announcement to Wednesday. Ne...       0      0     0   \n",
       "1                     @markpinc @TeslaMotors thanks!       0      0     0   \n",
       "2                           @Reuters Umm...Autobahn?       0      0     0   \n",
       "3                          @vicentes obviously wrong       0      0     0   \n",
       "4  @Cocoanetics @heiseonline Not actually based o...       0      0     0   \n",
       "\n",
       "   demonstr  lit  risk  vc  asshol  goal   ...    nerdsunit  signup  mute  \\\n",
       "0         0    0     0   0       0     0   ...            0       0     0   \n",
       "1         0    0     0   0       0     0   ...            0       0     0   \n",
       "2         0    0     0   0       0     0   ...            0       0     0   \n",
       "3         0    0     0   0       0     0   ...            0       0     0   \n",
       "4         0    0     0   0       0     0   ...            0       0     0   \n",
       "\n",
       "   raven  1julieanderson  procur  http  hottest  lschibi  bringi  \n",
       "0      0               0       0     0        0        0       0  \n",
       "1      0               0       0     0        0        0       0  \n",
       "2      0               0       0     0        0        0       0  \n",
       "3      0               0       0     0        0        0       0  \n",
       "4      0               0       0     0        0        0       0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = TextProcessor(tweets_class, 1000)\n",
    "processed = ex.process()\n",
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \"\"\"Constructs a NaiveBayes Classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        self.pP = None\n",
    "        self.pN = None\n",
    "        self.prob_p = None\n",
    "        self.prob_n = None\n",
    "        self.common_words = None\n",
    "    \n",
    "    def all_char(self, word):\n",
    "        for letter in word:\n",
    "            if not letter.isalnum():\n",
    "                return False\n",
    "        return True \n",
    "    \n",
    "    def stem_a_line(self, line,stemmed_words=None, stop_words=None):  \n",
    "        stemmer = PorterStemmer()\n",
    "        words = re.split(\"\\s\", line)\n",
    "        words = [word for word in words if (len(word) > 0 and self.all_char(word))]\n",
    "        stemmed_line = []\n",
    "        for word in words:\n",
    "            temp = word.replace(\"\\n\", \"\").lower()\n",
    "            if \"https\" not in temp and temp[0] != \"@\":\n",
    "                w = stemmer.stem(temp)\n",
    "                if w not in stopwords.words('english'):\n",
    "                    stemmed_line.append(w)\n",
    "                    if stemmed_words != None:                                    \n",
    "                        if w not in stemmed_words: stemmed_words.append(w)        \n",
    "                else:\n",
    "                     if stemmed_words != None:                                     \n",
    "                            if w not in stop_words: stop_words.append(w) \n",
    "        if stemmed_words != None:  \n",
    "            return(stemmed_words, stop_words, stemmed_line)\n",
    "        else:\n",
    "            return stemmed_line\n",
    "    \n",
    "    \"\"\"Generates vector of probabilities using NaiveBayes\n",
    "    parameters: vectorized_tweets, the dataframe of vectorized tweets\n",
    "                y, the sentimet of the tweets\"\"\"\n",
    "    def fit(self, vectorized_tweets,y):\n",
    "        den_positive = 0                     \n",
    "        den_negative = 0                 \n",
    "        for v in y:\n",
    "            if v == 0:\n",
    "                den_negative += 1\n",
    "            else:\n",
    "                den_positive += 1\n",
    "        common_words = vectorized_tweets.columns.tolist()\n",
    "        common_words = [word for word in common_words if word != \"all_tweets\"]\n",
    "        self.common_words = common_words\n",
    "        N = len(common_words)\n",
    "        prob_p = {}        \n",
    "        prob_n = {}\n",
    "        vect_n = np.zeros(N)\n",
    "        vect_p = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            for j in range(len(y)):\n",
    "                if y[j] == 0.0:\n",
    "                    vect_n[i] = (vect_n[i] + vectorized_tweets[common_words[i]][j])\n",
    "                else:\n",
    "                    vect_p[i] = (vect_p[i] + vectorized_tweets[common_words[i]][j])\n",
    "        for iwrd in range(N):\n",
    "            prob_n[common_words[iwrd]] = float(vect_n[iwrd])/den_negative    \n",
    "            prob_p[common_words[iwrd]] = float(vect_p[iwrd])/den_positive   \n",
    "        self.pP = float(den_positive)/(den_positive + den_negative)\n",
    "        self.pN = float(den_negative)/(den_positive + den_negative)\n",
    "        self.prob_p = prob_p\n",
    "        self.prob_n = prob_n\n",
    "    \n",
    "    \"\"\"Given a line, predict the sentiment, 1 or 0\n",
    "    parameters: line, the line to predict\n",
    "    return: result, the sentiment of the line\"\"\"\n",
    "    def predict(self, line):\n",
    "        stemmed_line = self.stem_a_line(line)\n",
    "        prob_p = self.pP\n",
    "        prob_n = self.pN\n",
    "        for w in sorted(set(stemmed_line)):   \n",
    "            if w in self.common_words:\n",
    "                prob_p *= self.prob_p[w]    \n",
    "                prob_n  *= self.prob_n[w]\n",
    "        if prob_n == 0.0 and prob_p == 0.0:\n",
    "            result = random.randint(0,1)\n",
    "        elif prob_n > prob_p:\n",
    "            result = 0\n",
    "        else:\n",
    "            result = 1\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(processed,tweets_class['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_tweets</th>\n",
       "      <th>extend</th>\n",
       "      <th>unkar</th>\n",
       "      <th>1949</th>\n",
       "      <th>demonstr</th>\n",
       "      <th>lit</th>\n",
       "      <th>risk</th>\n",
       "      <th>vc</th>\n",
       "      <th>asshol</th>\n",
       "      <th>goal</th>\n",
       "      <th>...</th>\n",
       "      <th>_push_superbowl</th>\n",
       "      <th>_blame_steelersn</th>\n",
       "      <th>_k12_superbowl</th>\n",
       "      <th>_litt_camcuss_say</th>\n",
       "      <th>_alert_200k</th>\n",
       "      <th>_could_derrickros</th>\n",
       "      <th>_miss_troubl_followu</th>\n",
       "      <th>_wearephx_releas</th>\n",
       "      <th>_option_insidehoop_buck</th>\n",
       "      <th>_cousin_seat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Moving the Tesla announcement to Wednesday Nee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>markpinc TeslaMotors thanks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reuters UmmAutobahn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vicentes obviously wrong</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cocoanetics heiseonline Not actually based on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1893 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          all_tweets  extend  unkar  1949  \\\n",
       "0  Moving the Tesla announcement to Wednesday Nee...       0      0     0   \n",
       "1                        markpinc TeslaMotors thanks       0      0     0   \n",
       "2                                Reuters UmmAutobahn       0      0     0   \n",
       "3                           vicentes obviously wrong       0      0     0   \n",
       "4  Cocoanetics heiseonline Not actually based on ...       0      0     0   \n",
       "\n",
       "   demonstr  lit  risk  vc  asshol  goal      ...       _push_superbowl  \\\n",
       "0         0    0     0   0       0     0      ...                     0   \n",
       "1         0    0     0   0       0     0      ...                     0   \n",
       "2         0    0     0   0       0     0      ...                     0   \n",
       "3         0    0     0   0       0     0      ...                     0   \n",
       "4         0    0     0   0       0     0      ...                     0   \n",
       "\n",
       "   _blame_steelersn  _k12_superbowl  _litt_camcuss_say  _alert_200k  \\\n",
       "0                 0               0                  0            0   \n",
       "1                 0               0                  0            0   \n",
       "2                 0               0                  0            0   \n",
       "3                 0               0                  0            0   \n",
       "4                 0               0                  0            0   \n",
       "\n",
       "   _could_derrickros  _miss_troubl_followu  _wearephx_releas  \\\n",
       "0                  0                     0                 0   \n",
       "1                  0                     0                 0   \n",
       "2                  0                     0                 0   \n",
       "3                  0                     0                 0   \n",
       "4                  0                     0                 0   \n",
       "\n",
       "   _option_insidehoop_buck  _cousin_seat  \n",
       "0                        0             0  \n",
       "1                        0             0  \n",
       "2                        0             0  \n",
       "3                        0             0  \n",
       "4                        0             0  \n",
       "\n",
       "[5 rows x 1893 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = TextProcessor(tweets_class, 1000)\n",
    "ex.process()\n",
    "ex.bigram(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the results into a data frame called \"Results\" then calculate the accuracy score and output the confusion matrix of this tweets_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1ad6e5e56113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mall_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassification\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_class' is not defined"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "classification = tweets_class.sentiment\n",
    "correct = []\n",
    "for i in range(len(tweets_class.text)):\n",
    "    line = tweets_class.text[i]\n",
    "    all_results.append(tweets_class.predict(line))\n",
    "    if nb.predict(line) == classification[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "results = pd.DataFrame({'prediction': all_results, 'classification': classification, 'correct': correct})\n",
    "print(float(sum(correct))/len(classification))\n",
    "false_p = 0\n",
    "false_n = 0\n",
    "true_p = 0\n",
    "true_n = 0\n",
    "for i in range(len(results)):\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 1:\n",
    "        true_p = true_p + 1\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 0:\n",
    "        false_n = false_n + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 1:\n",
    "        false_p = false_p + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 0:\n",
    "        true_n = true_n + 1\n",
    "c_matrix = np.matrix([[true_p,false_p],[false_n,true_n]])\n",
    "print(c_matrix)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add bigrams to the tweets_class and recreate the Results data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed = ex.bigram(2)\n",
    "nb = NaiveBayes()\n",
    "nb.fit(processed,tweets_class['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6156770985668324\n",
      "[[ 650  279]\n",
      " [1043 1445]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>correct</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classification  correct  prediction\n",
       "0             0.0        0           1\n",
       "1             1.0        1           1\n",
       "2             0.0        1           0\n",
       "3             0.0        1           0\n",
       "4             0.0        1           1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "classification = tweets_class.sentiment\n",
    "correct = []\n",
    "for i in range(len(tweets_class.text)):\n",
    "    line = tweets_class.text[i]\n",
    "    all_results.append(nb.predict(line))\n",
    "    if nb.predict(line) == classification[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "results = pd.DataFrame({'prediction': all_results, 'classification': classification, 'correct': correct})\n",
    "print(float(sum(correct))/len(classification))\n",
    "false_p = 0\n",
    "false_n = 0\n",
    "true_p = 0\n",
    "true_n = 0\n",
    "for i in range(len(results)):\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 1:\n",
    "        true_p = true_p + 1\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 0:\n",
    "        false_n = false_n + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 1:\n",
    "        false_p = false_p + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 0:\n",
    "        true_n = true_n + 1\n",
    "c_matrix = np.matrix([[true_p,false_p],[false_n,true_n]])\n",
    "print(c_matrix)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Train-Test split \n",
    "\n",
    "Now, before you build your model, you need to construct some datasets to try out.  Fortunately, we've been working on building one!  You can download `tweets_class.txt` from Canvas.  It's a `.txt`, not a `.csv`, because tweets have a lot of commas in them, and that messed with Pandas `read_csv` method.  So `tweets_class.txt` is a tab-delimited dataset.  You'll need to change the delimiter for `read_csv`.  I also found that including the keyword `encoding='ISO-8859-1'` helped a ton as well.\n",
    "\n",
    "Take the data, split off some amount of it as a **test dataset**.  This means that you don't give it to your model, but you run it through the model after training to test its accuracy on tweets it hasn't seen before.  How much to split off is a good question, and the numbers vary from 10% to 50%.  Both of those extremes I think are a little over-the-top, I would recommend about 20-30%.  The remaining dataset is called your **training dataset**.\n",
    "\n",
    "A quick google search allowed me to find the gigantic [Sentiment Analysis Dataset](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip).  It's 150MB, consisting of 1.5 million tweets which have been labeled for sentiment.  You may be getting frustrated with me at this point: \"Why did we have to go label all those tweets when there's a huge dataset already?!\"  I have many answers to this, but the most important one is that all of these labeled tweets were labeled by someone else's model, so I didn't want you to work off of entirely computer-generated data (It's the same idea behind \"a copy of a copy of a copy...\".  Nonetheless, let's pad our dataset with it.  Here's a line I found useful for opening that massive dataset in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Sentiment Analysis Dataset.csv\", encoding='ISO-8859-1', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some number of these labeled tweets to your dataset.  How much is up to you, I might suggest some number of thousands, but less than 10 thousand (Though feel free to try more!).  Take them randomly from the dataframe, not from the top.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                                text  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['ItemID', 'sentiment', 'SentimentSource', 'text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take 1% of the original sentiment analysis dataset, then split it into testing data and training data. Add the training data to the original tweets class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Moving the Tesla announcement to Wednesday Nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>markpinc TeslaMotors thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Reuters UmmAutobahn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>vicentes obviously wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Cocoanetics heiseonline Not actually based on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0        0.0  Moving the Tesla announcement to Wednesday Nee...\n",
       "1        1.0                        markpinc TeslaMotors thanks\n",
       "2        0.0                                Reuters UmmAutobahn\n",
       "3        0.0                           vicentes obviously wrong\n",
       "4        0.0  Cocoanetics heiseonline Not actually based on ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_n = df[0:int(0.001*len(df))]\n",
    "test_data = df_n[0:int(0.3*len(df_n))]\n",
    "training_data = df_n[int(0.3*len(df_n)):]\n",
    "all_classifications = [sentiment for sentiment in tweets_class.sentiment] + [sentiment for sentiment in training_data.sentiment]\n",
    "all_text =  [text for text in tweets_class.text]+[text for text in training_data.text]\n",
    "data = pd.DataFrame({'text': all_text, 'sentiment':all_classifications})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Train and test your models\n",
    "\n",
    "Build both a Naive Bayes and a Logistic Regression classifier on your training dataset, and test them on your test dataset.  Which is better? What percent of positives and negatives do you have? How many false positives and false negatives do you have?  Interpret your results.  Is your model better or worse when you include the computer-generated data?  Add some bigrams to, and try changing your `N` in Naive Bayes, and try changing your `C` in Logistic Regression.  What's the best model? (This is why creating a robust class in part 1 will help you.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the training data\n",
    "Re-fit using NaiveBayes for the new training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized_tweets = TextProcessor(data, 1000).process()\n",
    "nbayes_tweets = NaiveBayes()\n",
    "nbayes_tweets.fit(vectorized_tweets, data.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying testing data and returning accuracy score, confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6511627906976745\n",
      "[[  9  21]\n",
      " [147 296]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>correct</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classification  correct  prediction\n",
       "0               0        1           0\n",
       "1               0        1           0\n",
       "2               1        0           0\n",
       "3               0        1           0\n",
       "4               0        1           0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "classification = test_data.sentiment\n",
    "correct = []\n",
    "for i in range(len(test_data.text)):\n",
    "    line = test_data.text[i]\n",
    "    all_results.append(nbayes_tweets.predict(line))\n",
    "    if nbayes_tweets.predict(line) == classification[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "results = pd.DataFrame({'prediction': all_results, 'classification': classification, 'correct': correct})\n",
    "print(float(sum(correct))/len(classification))\n",
    "false_p = 0\n",
    "false_n = 0\n",
    "true_p = 0\n",
    "true_n = 0\n",
    "for i in range(len(results)):\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 1:\n",
    "        true_p = true_p + 1\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 0:\n",
    "        false_n = false_n + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 1:\n",
    "        false_p = false_p + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 0:\n",
    "        true_n = true_n + 1\n",
    "c_matrix = np.matrix([[true_p,false_p],[false_n,true_n]])\n",
    "print(c_matrix)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding bigrams\n",
    "Re-fit the model with the bigrams added and re-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex = TextProcessor(data, 1000)\n",
    "ex.process()\n",
    "processed = ex.bigram(2)\n",
    "nb = NaiveBayes()\n",
    "nb.fit(processed,data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642706131078224\n",
      "[[ 11  19]\n",
      " [145 298]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>correct</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classification  correct  prediction\n",
       "0               0        1           0\n",
       "1               0        1           0\n",
       "2               1        0           0\n",
       "3               0        1           0\n",
       "4               0        1           0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "classification = test_data.sentiment\n",
    "correct = []\n",
    "for i in range(len(test_data.text)):\n",
    "    line = test_data.text[i]\n",
    "    all_results.append(nb.predict(line))\n",
    "    if nb.predict(line) == classification[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "results = pd.DataFrame({'prediction': all_results, 'classification': classification, 'correct': correct})\n",
    "print(float(sum(correct))/len(classification))\n",
    "false_p = 0\n",
    "false_n = 0\n",
    "true_p = 0\n",
    "true_n = 0\n",
    "for i in range(len(results)):\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 1:\n",
    "        true_p = true_p + 1\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 0:\n",
    "        false_n = false_n + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 1:\n",
    "        false_p = false_p + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 0:\n",
    "        true_n = true_n + 1\n",
    "c_matrix = np.matrix([[true_p,false_p],[false_n,true_n]])\n",
    "print(c_matrix)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "Get all the words in common words and then make sure no values are NaN or infinite. Use LogisticRegression() to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   extend  unkar  rod  1949  demonstr  lit  risk  jm  vc  asshol     ...       \\\n",
      "0       0      0    0     0         0    0     0   0   0       0     ...        \n",
      "1       0      0    0     0         0    0     0   0   0       0     ...        \n",
      "2       0      0    0     0         0    0     0   0   0       0     ...        \n",
      "3       0      0    0     0         0    0     0   0   0       0     ...        \n",
      "4       0      0    0     0         0    0     0   0   0       0     ...        \n",
      "\n",
      "   _bore_talk  _rain_gone  _take_ride  _month_talk  _till_eat_wait  \\\n",
      "0           0           0           0            0               0   \n",
      "1           0           0           0            0               0   \n",
      "2           0           0           0            0               0   \n",
      "3           0           0           0            0               0   \n",
      "4           0           0           0            0               0   \n",
      "\n",
      "   _happi_hous  _sink_must  _ew_gt_came  _could_sync  _miss_month  \n",
      "0            0           0            0            0            0  \n",
      "1            0           0            0            0            0  \n",
      "2            0           0            0            0            0  \n",
      "3            0           0            0            0            0  \n",
      "4            0           0            0            0            0  \n",
      "\n",
      "[5 rows x 1808 columns]\n"
     ]
    }
   ],
   "source": [
    "col = [n for n in processed.columns.values if n is not \"all_tweets\"]\n",
    "X = processed[col]\n",
    "y=data.sentiment\n",
    "logreg = LogisticRegression()\n",
    "isnan_idx = []\n",
    "for i in range(len(y)):\n",
    "    if isnan(y[i]):\n",
    "        isnan_idx.append(i)\n",
    "y = y.drop(y.index[[isnan_idx]])\n",
    "X = X.drop(X.index[[isnan_idx]])\n",
    "model = logreg.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jocel\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6596194503171248\n",
      "c_matrix\n",
      "[[  2   7]\n",
      " [154 310]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>correct</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classification  correct  prediction\n",
       "0               0        1           0\n",
       "1               0        1           0\n",
       "2               1        0           0\n",
       "3               0        1           0\n",
       "4               0        1           0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = []\n",
    "for lin in test_data.text:\n",
    "    stemmed.append(stem_a_line(lin))\n",
    "test_data['stemmed_text'] = stemmed\n",
    "all_results = []\n",
    "classification = test_data.sentiment\n",
    "correct = []\n",
    "for i in range(len(test_data.text)):\n",
    "    text = test_data.stemmed_text[i]\n",
    "    arr = [1 if word in text else 0 for word in col]\n",
    "    a = pd.DataFrame(arr)\n",
    "    \n",
    "    all_results.append(int(round(model.predict(a.values.reshape(1,-1))[0])))\n",
    "    if int(round(model.predict(a.values.reshape(1,-1))[0])) == classification[i]:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "results = pd.DataFrame({'prediction': all_results, 'classification': classification, 'correct': correct})\n",
    "print(float(sum(correct))/len(classification))\n",
    "false_p = 0\n",
    "false_n = 0\n",
    "true_p = 0\n",
    "true_n = 0\n",
    "for i in range(len(results)):\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 1:\n",
    "        true_p = true_p + 1\n",
    "    if results['classification'][i] == 1 and results['prediction'][i] == 0:\n",
    "        false_n = false_n + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 1:\n",
    "        false_p = false_p + 1\n",
    "    if results['classification'][i] == 0 and results['prediction'][i] == 0:\n",
    "        true_n = true_n + 1\n",
    "c_matrix = np.matrix([[true_p,false_p],[false_n,true_n]])\n",
    "print(\"c_matrix\")\n",
    "print(c_matrix)\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
